https://github.com/GoogleCloudPlatform/microservices-demo

https://github.com/microservices-demo/microservices-demo

https://github.com/docshome/microservices

What is Container:
===================
A container is a unit of software that packages code and its dependencies so the application runs quickly and reliably across computing environments.

Containers are packages of software that contain all of the necessary elements to run in any environment. In this way, 
containers virtualize the operating system and run anywhere, from a private data center to the public cloud or even on a developer's personal laptop.


While the promise of containers is to code once and run anywhere, Kubernetes provides the potential to orchestrate and 
manage all your container resources from a single control plane. It helps with networking, load-balancing, security, 
and scaling across all Kubernetes nodes which runs your containers.


https://enterprisersproject.com/article/2020/9/pod-cluster-container-what-is-difference

======================================================================================================================================

steps:
=====

1. src .
2. Toconfig --> Dockerfile
3. Build ----> Image
4. Image ---> Repo
5. K8s ----> 1. deploy.yaml
             2. Service.yaml



1. source: clone repo

https://github.com/devopswithcloud/testingrepo

https://github.com/devopswithcloud/testingrepo.git

cd testingrepo/
2. config the Dockerfile
3. Build the docker image
docker build -t rmspavan/sample_py_flask:ocpdemo .
4. push the docker image
docker push rmspavan/sample_py_flask:ocpdemo
5. deploy to k8s

kubectl create deploy ocpdemo --image rmspavan/sample_py_flask:ocpdemo --dry-run=client -o yaml > deploy.yaml

kubectl apply -f deploy.yaml
kubectl get pods -A
kubectl expose deploy ocpdemo --port 8080 --target-port 8080 --type NodePort
kubectl get svc -A

http://ip:nodeport

# In OCP just clone the repo and click build... 

why k8s?

1. can't live single server for deployment
2. container to container
3. Autoscaling
4. Autohealing

Kubernetes is an open-source container orchestration system for automating software deployment, scaling, and management

APISERVER: Apiserver is the responsible for orchestration all the operation with in your cluster, it will behave like frontend

The Kubernetes API server validates and configures data for the api objects which include pods, services, replicationcontrollers, and others. 
The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact

The API (application programming interface) server determines if a request is valid and then processes it. In essence, 
the API is the interface used to manage, create, and configure Kubernetes clusters. It's how the users, external components, 
and parts of your cluster all communicate with each other.

ETCD:

etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data. 
If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data. 
You can find in-depth information about etcd in the official documentation.

Kubernetes uses etcd to store all its data â€“ its configuration data, its state, and its metadata. Kubernetes is a distributed system, 
so it needs a distributed data store like etcd. etcd lets any of the nodes in the Kubernetes cluster read and write data.

Scheduler:

One controller consults the scheduler and makes sure the correct number of pods is running. If a pod goes down, 
another controller notices and responds. A controller connects services to pods, so requests go to the right endpoints. 
And there are controllers for creating accounts and API access tokens.

In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that Kubelet can run them.

kube-controller-manager:
The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. 
In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. 
In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes 
attempting to move the current state towards the desired state. 
Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.


1. Admin/Developer
kubectl or oc(tool)
2. API server
3. apiserver --> etcd
4. etcd --> apiserver
5. apiserver--> scheduler
6. scheduler--> (API or advance schd)
7. schdeduler --> APiserver
8. Apiserver  --> kubelet
9. kubelet --> CRT (docker/cri-o,Containerd)
10. crt --> container
11. container -->pod
12. kubelet ---> apiserver --> storing the data to etcd service


Openshit can deploy in 
    - physical machines
    - Virtual machines
    - private cloud
    - public cloud


-----------------------------------------------------------------------------------------------------------------------------------------------------------------
=================================================================================================================================================================

CRC-install on minimum requirements:
=====================================

https://developers.redhat.com/blog/2020/09/09/install-red-hat-openshift-operators-on-your-laptop-using-red-hat-codeready-containers-and-red-hat-marketplace#

https://developers.redhat.com/blog/2019/09/05/red-hat-openshift-4-on-your-laptop-introducing-red-hat-codeready-containers

https://access.redhat.com/documentation/en-us/red_hat_openshift_local/2.16/html/getting_started_guide/installation_gsg


# Create cluster:
===================

openshift features:
    - HA
    - Light weight os (core OS)
    - load balancing
    - Automatic scaling (pods, Nodes)
    - Service discovery
    - Storage
    - UI
    - CICD

Day1 tasks: cluster setup, provisioning and infra provisioning
Day2 tasks: Administartion, Developmnt activities, deploy apps.
            to verify logs/pod crash, pipeline implementation, build images, App onboard, volumes.


# Cluster deployment types:
 - redhat OP dedicated
 - redhat op online
 - redhat OCP
 - OKD
 - Origin
 - CRC
 - managed k8s 

 
 OCP (openshift container platform):
 ===================================

 Components required:
  - AWS account
  - Doiman
  - EC2 machine with keys (1. ec2, 2. keygen(users))
  - Account in RedHat
  - Subscription in openshift
  - OCP installer
  - OC client (to cnnect api)
  - have to know what happening in background



  ## k8s-hardway
https://github.com/Praqma/LearnKubernetes/blob/master/kamran/Kubernetes-The-Hard-Way-on-BareMetal.md

https://medium.com/@DrewViles/kubernetes-the-hard-way-on-bare-metal-vms-v1-23-2168f5fe70af

https://faun.pub/configuring-ha-kubernetes-cluster-on-bare-metal-servers-with-kubeadm-1-2-1e79f0f7857b



# create Aws account
# Create doamin in (godaddy/nacecheap) for free doamin (freenom)
  - link the domain in AWS Route53 with NS.
    
# EC2 machine 
   - Creating a Jump server in ohio(us-east-2), t2.micro, Amazon Linux 64bit with keypair.
   - login to jump_server and enable root users (usermod -aG wheel pavan)
   - 

# Account in Redhat
  - developers.redhat.com (create account)--> join redhat developer in below
  - subscription for ocp  
  - console.redhat.com (openshift--> create cluster: cloud/Datacenter/local)
       - cloud --> Managed Server/ Run it yourself
			- Run it yoursef: AWS cloud  AWS(x86_64)
			- IPI(installer provisioned infra) or UPI(user provision infra) 
            - select IPI and download installer and oc client tool (installer copy to jump_server or copy url)
			- go to jump_server
			   mkdir ocp
			   wget <url of openshift installer)
			   tar xvzf openshift.tar.gz

# install oc client tool
	wget <url of oc client tool installer) 
	tar xvzf opensift-clint-linux.ta.gz
	cp oc kubectl /usr/bin/ (copy oc and  kubectl in env path)
	
# generate install-config.yaml ( prereq: IAM user with access key, secreat key)
	./openshift-install create --help
	aws configure
		- Access key:
		- secret key:
		- region name:
        - output format: json
	to test: aws s3 ls
    ./openshift-install create install-config --dir=/root/ocp
		> select aws
		> select region
		> select the domain
		> cluster Name: ipi
		> pull secret (copy the secret from IPI and paste)
	- now install-config.yaml file will generate
	vi install-config.yaml
		change based on your requirement like replicas: master, worker, network.
		
before create install-config.yaml
===================================

generate ssh key pait on your local machine to use for authentication onto your cluster nodes
$ ssh-keygen -t ed25519 -N '' -f .ssh/id_rsa
$ ll .ssh

create install-config.yaml file
$ ./openshift-install create install-config --dir=/root/ocp
    > SSH public key
    > aws
    > us-east-2
    > domain
    > cluster name
    > pull secret (paste)


$ vi install-config.yaml (change based on your requirement)
   compute:
   - architucture: amd64
     plaform:
        aws:
          type: m5.xlarge (if not provide by default m6.xlarge)
     replicas: 1 

note:
-----
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/installing/installing-on-aws#installing-aws-customizations

Table 4.5. Minimum resource requirements
Machine         OS      vCPU  RAM   Storage IOPS 
Bootstrap       RHCOS   4     16GB  100GB   300
Controlplane    RHCOS   4     16GB  100GB   300
Compute     RHCOS,RHEL  2     8GB  100GB   300


Create cluster.yaml
====================
./openshift-install create cluster --dir=/root/ocp --log-level=info
it will take 30 to 40mins to create cluster

tail -f .openshift_install.log
    - will provide openshift cluster url, username and password

to destroy cluster:
===================
./openshift-install destroy cluster --dir=/root/ocp --log-level=info

==================================================================================================================================================================

	

How to connent Openshift
	- console (using route(url) with username and password)
	- cli (oc client: oc 
		: cat .openshift_install.log | grep -i export (it will give the steps to access cluster, create config file)
		     config file: export KUBECONFIG=/root/ocp/auth/kubconfig
						  cp /root/ocp/auth/kubconfig ~/.kube/config

 	     :oc get nodes
         :oc get node --kubeconfig /root/ocp/kubeconfig (alternative)
			
Projets
pods
containers
controller Manager
	- replication controller
	- replication set
	- deployment
	- deployment config
	

ocp have multiple clusters like Dev/Test/nonprod/prod
	- admins
	- developers
	- app developers

OCP by default user will create "kubeadmin" this user have all the rights
	
# connect form laptops:

	install oc client
    cat .openshift_install.log | grep 6443(api url)
	oc login <url>:6443 -u <userid> -p <password>
	or 
	oc login --token=<token> --server=<url>:6443
	
Setp:1
======
1. have to create project (in k8s called as namespace), it will divided logically
   Project have resource/object
   In one project have multiple resouces
   
   oc help
   oc create help
   oc projects
   oc get routes -n openshift-console
   
   #Create a new-project
	oc new-project --help
	oc new-project <project-name> --description="<dev application>" --display-name="<aws dev>"
	
   #to switch project
     oc project <project-name>
	 oc project (to check project)
	 
	


    	
 


































